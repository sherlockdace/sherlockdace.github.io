<!doctype html>

































<html
  class="not-ready lg:text-base"
  style="--bg: #fff"
  lang="en-us"
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>Quantization in LLMs - Morphism42</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="As large language models (LLMs) continue to expand in size and intricacy, mitigating their computational and energy demands has emerged as a pivotal challenge. Quantization has surfaced as a prominent approach, involving the reduction of parameter precision in models to lower bit formats. For instance, numerous methodologies truncate parameters from the conventional 16-bit floating point (FP16) or 32-bit floating point (FP32) to diminished bit formats such as 8-bit or 4-bit." />
  <meta name="author" content="Morphism42" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="http://localhost:1313/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="http://localhost:1313/theme.svg" />

  
  
  
  
  

  
  
  <link rel="preload" as="image" href="http://localhost:1313/github.svg" />
  
  

  
  

  
  
  
  
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
  integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
  integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
  integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
  crossorigin="anonymous"
></script>


<script>
  document.addEventListener('DOMContentLoaded', () =>
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
      ],
      
      throwOnError: false,
    }),
  );
</script>

  
  
  

  
  <link
    rel="icon"
    href="http://localhost:1313/favicon.ico"
  />
  <link
    rel="apple-touch-icon"
    href="http://localhost:1313/apple-touch-icon.png"
  />

  
  <meta name="generator" content="Hugo 0.128.0">

  
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a class="-translate-y-[1px] text-2xl" href="http://localhost:1313/"
      >Morphism42</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#fff'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-14 lg:mt-0 lg:items-center"
    >
      
      <a
        class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/sherlockdace"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-14 dark:prose-invert"
    >
      

<article>
  <header class="mb-14">
    <h1 class="!my-0 pb-2.5">Quantization in LLMs</h1>

    
    <div class="text-xs antialiased opacity-60">
      
      <time>Oct 3, 2024</time>
      
      
      
      
    </div>
    
  </header>

  <section><p>As large language models (LLMs) continue to expand in size and intricacy, mitigating their computational and energy demands has emerged as a pivotal challenge. Quantization has surfaced as a prominent approach, involving the reduction of parameter precision in models to lower bit formats. For instance, numerous methodologies truncate parameters from the conventional 16-bit floating point (FP16) or 32-bit floating point (FP32) to diminished bit formats such as 8-bit or 4-bit.</p>
<p>In this article, our focus will be on particular quantization models, delving into the fundamental motivations and concepts underpinning these endeavors.</p>
<h2 id="1-bit">1-BIT</h2>
<p>The concept of 1-bit estimation, as proposed in the <a href="https://arxiv.org/pdf/2310.11453">1-bit LLM paper</a>, introduces a novel approach towards quantization in LLMs.</p>
<p>In traditional LLM models, a fundamental operation involves the linear operator <code>nn.Linear</code>. In essence, a linear operator $f$ transforms an input $X$ into an output $Y$ through the operation $f(X) := WX$, where $W$ represents a tensor. Typically, in a generic linear operator, the entries of the weight tensor $W$ are real-valued numbers. However, in the context of 1-bit quantization models, the entries are constrained to the set ${ -1, 0, 1 }$, significantly reducing the computational cost of the operation $f(X)$ for LLMs. This reduction in computational cost is a key advantage of the 1-bit quantization scheme, offering potential efficiency gains in the processing of Large Language Models.</p>
<p><img src="https://github.com/sherlockdace/sherlockdace.github.io/blob/main/content/imgs/llm_quant/llm_quant_1.jpg?raw=true" alt="1-bit quantization linear operator vs FP16 linear operator"></p>
<p>For the 1-bit model, the input $X$ and the matrix $W$ are both transferred into the integer forms. Specifically, the matrix $W$ is modified according to the following rule:
$$
scale_w = \frac{1}{mn \sum_{i, j} |W_{ij}|} ,
$$
$$
W_q = clamp_{[-1, 1]} (round(W * scale_w)),
$$
$$
W_{dequantized} = W_q * scale_w.
$$</p>
<p>On the other side, the input $X$ is also required to be modified into integer numbers. The quantization formula is
$$
scale_x = \frac{127}{|X|_{max, dim=-1}} ,
$$</p>
<p>$$
X_q = clamp_{[-128, 127]} (round (X * scale_x)),
$$
$$
X_{dequantized} = X_q * sacle_x.
$$</p>
<p>Based on this idea, the python code implementation of the 1-bit model is given as follows.
Let $LN (x):= \frac{x - \mathbb{E} (x)}{\sqrt{Var(x)}}$ be the normalization of the input $X$.
Then, we have</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Adapted from https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn 
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">activation_quant</span>(x):
</span></span><span style="display:flex;"><span>    scale <span style="color:#f92672">=</span> <span style="color:#ae81ff">127.0</span> <span style="color:#f92672">/</span> x<span style="color:#f92672">.</span>abs()<span style="color:#f92672">.</span>max(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>clamp_(min<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-5</span>)
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> (x <span style="color:#f92672">*</span> scale)<span style="color:#f92672">.</span>round()<span style="color:#f92672">.</span>clamp_(<span style="color:#f92672">-</span><span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">127</span>) <span style="color:#f92672">/</span> scale
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> y
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">weight_quant</span>(w):
</span></span><span style="display:flex;"><span>    scale <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> w<span style="color:#f92672">.</span>abs()<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>clamp_(min<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-5</span>)
</span></span><span style="display:flex;"><span>    u <span style="color:#f92672">=</span> (w <span style="color:#f92672">*</span> scale)<span style="color:#f92672">.</span>round()<span style="color:#f92672">.</span>clamp_(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> scale
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> u
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BitLinear</span>(nn<span style="color:#f92672">.</span>Linear):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Only for training
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        w <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>        x_norm <span style="color:#f92672">=</span> LN(x)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># A trick for implementing Straight−Through−Estimator (STE) using detach()</span>
</span></span><span style="display:flex;"><span>        x_quant <span style="color:#f92672">=</span> x_norm <span style="color:#f92672">+</span> (activation_quant(x_norm) <span style="color:#f92672">-</span> x_norm)<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>        w_quant <span style="color:#f92672">=</span> w <span style="color:#f92672">+</span> (weight_quant(w) <span style="color:#f92672">-</span> w)<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Perform quantized linear transformation</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>linear(x_quant, w_quant)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y
</span></span></code></pre></div></section>

  
  

  
  

  
  

  
  

  


  
</article>


    </main>

    <footer
  class="mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-xs uppercase tracking-wider opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="http://localhost:1313/">Morphism42</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>

  </body>
</html>
