<!doctype html>

































<html
  class="not-ready lg:text-base"
  style="--bg: #fff"
  lang="en-us"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>The Magnitude Level of First-Order Methods - Morphism42 Blog</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="In broad terms, the majority of training problems we encounter involve the task of locating a global or local minimum of a given function $f(\theta)$. Within the field of deep learning, $\theta$ represents the parameter values to be determined for a given neural network structure. Numerous approaches, particularly first-order methods, have been devised to identify the local or global optimal minimum point $\theta^*$. In this blog, we aim to delve into the discussion surrounding the magnitude level of the correction term in these first-order methods." />
  <meta name="author" content="Morphism42 Blog" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://sherlockdace.github.io/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="https://sherlockdace.github.io/theme.svg" />

  
  
  
  
  

  
  
  <link rel="preload" as="image" href="https://sherlockdace.github.io/github.svg" />
  
  

  
  

  
  
  
  
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
  integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
  integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
  integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
  crossorigin="anonymous"
></script>


<script>
  document.addEventListener('DOMContentLoaded', () =>
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
      ],
      
      throwOnError: false,
    }),
  );
</script>

  
  
  

  
  <link rel="icon" href="https://sherlockdace.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://sherlockdace.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.111.3">

  
  
  
  
  
  <meta itemprop="name" content="The Magnitude Level of First-Order Methods">
<meta itemprop="description" content="In broad terms, the majority of training problems we encounter involve the task of locating a global or local minimum of a given function $f(\theta)$. Within the field of deep learning, $\theta$ represents the parameter values to be determined for a given neural network structure. Numerous approaches, particularly first-order methods, have been devised to identify the local or global optimal minimum point $\theta^*$. In this blog, we aim to delve into the discussion surrounding the magnitude level of the correction term in these first-order methods."><meta itemprop="datePublished" content="2024-06-29T20:08:01+08:00" />
<meta itemprop="dateModified" content="2024-06-29T20:08:01+08:00" />
<meta itemprop="wordCount" content="232">
<meta itemprop="keywords" content="" />
  
  <meta property="og:title" content="The Magnitude Level of First-Order Methods" />
<meta property="og:description" content="In broad terms, the majority of training problems we encounter involve the task of locating a global or local minimum of a given function $f(\theta)$. Within the field of deep learning, $\theta$ represents the parameter values to be determined for a given neural network structure. Numerous approaches, particularly first-order methods, have been devised to identify the local or global optimal minimum point $\theta^*$. In this blog, we aim to delve into the discussion surrounding the magnitude level of the correction term in these first-order methods." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sherlockdace.github.io/post/gd_level/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-06-29T20:08:01+08:00" />
<meta property="article:modified_time" content="2024-06-29T20:08:01+08:00" />

  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="The Magnitude Level of First-Order Methods"/>
<meta name="twitter:description" content="In broad terms, the majority of training problems we encounter involve the task of locating a global or local minimum of a given function $f(\theta)$. Within the field of deep learning, $\theta$ represents the parameter values to be determined for a given neural network structure. Numerous approaches, particularly first-order methods, have been devised to identify the local or global optimal minimum point $\theta^*$. In this blog, we aim to delve into the discussion surrounding the magnitude level of the correction term in these first-order methods."/>

  
  
  
  <link rel="canonical" href="https://sherlockdace.github.io/post/gd_level/" />
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold"
      href="https://sherlockdace.github.io"
      >Morphism42 Blog</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#fff'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-12 lg:mt-0 lg:items-center lg:space-x-6"
    >
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/sherlockdace"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">The Magnitude Level of First-Order Methods</h1>

    
    <div class="text-sm antialiased opacity-60">
      
      <time>Jun 29, 2024</time>
      
      
      
      
    </div>
    
  </header>

  <section><p>In broad terms, the majority of training problems we encounter involve the task of locating a global or local minimum of a given function $f(\theta)$.
Within the field of deep learning, $\theta$ represents the parameter values to be determined for a given neural network structure.
Numerous approaches, particularly first-order methods, have been devised to identify the local or global optimal minimum point $\theta^*$.
In this blog, we aim to delve into the discussion surrounding the magnitude level of the correction term in these first-order methods.</p>
<h2 id="some-classical-first-order-methods">Some Classical First-Order Methods</h2>
<p>Before we delve into a formal discussion, it&rsquo;s worth noting that many first-order optimization methods can be expressed iteratively using the following form:
$$\theta_{k+1} = \theta_k - \alpha p_k. \tag{1.1} $$
In this equation, $\theta_k$ represents the current parameter, $\alpha$ is the learning rate, and $p_k$ denotes the correction term added to the current parameter $\theta_k$.</p>
<p>Notably, widely used optimization methods such as <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGD</a> and <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam</a> can be viewed as special cases of equation (1.1).
In SGD, the correction term $p_k$ is equivalent to the gradient term $\nabla_\theta f(\theta)$, where $\nabla_\theta$ represents the gradient with respect to the parameters $\theta$, and $f(\theta)$ denotes the objective function being optimized.
In contrast, Adam utilizes a different form of the correction term. Specifically, Adam computes the correction term as $m_k / \sqrt{v_k}$, where $m_k$ denotes the first momentum term and $v_k$ represents the second momentum term.</p>
</section>

  
  

  
  

  
  

  
  

  


  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="https://sherlockdace.github.io">Morphism42 Blog</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >✎ Paper</a
  >
</footer>

  </body>
</html>
